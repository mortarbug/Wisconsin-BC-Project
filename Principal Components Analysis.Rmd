---
title: "Principal Components Analysis RMD"
author: "Patrick Vo"
date: "12/6/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In the initial exploration, we found that the data exhibited collinearity, that only 11 variables were really important to the random forest model, and that just 2 variables were already enough to make a pretty good discriminating split.

A dimensionality reduction technique might allow us to examine the variance in the data further and to try to combat some of the collinearity. I'll try to run a principal components analysis on the wbc dataset.

```{r Setup and preparation}
### Principal Components Analysis on Variables 
setwd('C:/Users/MBG/Google Drive/Academic Work/Wisconsin Breast Cancer')
library(ggplot2)

# Read in the dataset 
wbc <- as.matrix(read.csv('wisc_bc_data.csv'))


# Remove the ID column and separate the response column from the rest of the matrix
#    After ID and diagnosis have been removed, convert the matrix to numerics
wbc <- wbc[,-1]
diagnosis <- wbc[,1]
wbc<- wbc[,-1]
wbc <- apply(wbc, 2, FUN = as.numeric)

```

```{r}
# Center and scale the matrix according to column. This should end with mu = 0, sd = 1
wbc <-  scale(wbc, center = TRUE, scale = TRUE)

# Find the eigendecomposition of the variance matrix and save the values and vectors
egn <- eigen(var(wbc))
combo_coefficients <- egn$vectors
variance_explained <- egn$values
```

Let's have a look at the SCREE plot first. I'm looking specifically for an "elbow" of some sort, where the percentage of variance explained drops dramatically. 

```{r}
# Print a SCREE plot of the variance explained
par(mfrow = c(1,1))
percent_variance_explained <- variance_explained/sum(variance_explained)
x <- (1:length(variance_explained))

# SCREE Plot 
ggplot() + 
  geom_point(aes(x=x, y = percent_variance_explained)) +
  labs(title = 'SCREE Plot', x = 'Principal Components')
```

```{r}
# Print the percent of variance explained corresponding to the first four principal components
percent_variance_explained[1:4]

# Give the total percent of variance explained by the first four pcs
sum(percent_variance_explained[1:4])

# Print the percent of variance explained corresponding to the first four principal components
percent_variance_explained[1:6]

# Give the total percent of variance explained by the first four pcs
sum(percent_variance_explained[1:6])



```

It looks like the SCREE plot reaches an elbow around the fourth principal component. All combined, four principal components are enough to account for 79% of the variance. By the sixth principal component, 88% of the variance has been captured. 

I want to have a look at a plot of how the diagnoses look like when plotted with the first and the second principal components.  

```{r}
# Find the first two principal components 
PCs <- wbc %*% combo_coefficients[,1:2]

# Combine the matrix of principal components with the vector of responses
PCs <- data.frame(cbind(PCs, diagnosis))
names(PCs) <- c('PC1', 'PC2', 'Diagnosis')


# Convert the first two principal components into numerics instead of factors
PCs$PC1 <- as.numeric(PCs$PC1)
PCs$PC2 <- as.numeric(PCs$PC2)

# Create a plot by color of the first two principal components 
ggplot(PCs, aes(x = PC1,y = PC2)) + 
  geom_point(aes(col = Diagnosis)) 
```


The first two principal components show that the response classes largely fall into two pretty well-defined areas. If we add more principal components in, we may be able to achieve complete or near-complete separability. When fitting models to the data, we'll have to take this into account, as some (logistic regression in particular) are sensitive to separability. 

The first principal component seems to be especially significant--the classes seem to fall on a either side of a vertical dividing line that can be constructed from a certain value of PC1. 

Let's have a look at the loadings of the first two principal components to see what kinds of patterns we can detect. 

```{r PC1}
# Find the eigenvector that went into PC1
print(egn$vectors[,1])

# Group Find the names corresponding to the 15 largest values of PC1
wbc_names <- colnames(wbc)
wbc_names[order(egn$vectors[,1])][1:15]
```

The coefficients of the first principal component all have the same sign--the first PC can be thought of as a weighted sum of the predictor variables. 

Looking at the names of the variables with the highest loading, we can see that many of the variables we found to be important in the initial exploration are showing up here--concave.points_mean, area_worst, radius_worse, etc... It's likely that the size of the nucelus in general is a pretty good predictor of whether or not the cell is cancerous.

I'll have a look at the second PC as well, usin the same methods as above.

```{r PC2}
# Find the eigenvector that went into PC2
print(egn$vectors[,2])

# Group Find the names corresponding to the 14 values with the highest positive and negative values of PC2
wbc_names[order(egn$vectors[,2])][1:7]
wbc_names[order(egn$vectors[,2])][23:30]
```

PC2 has different signs, and functions as a kind of contrast between variables. We can see that the negative loadings correspond to values that mostly describe size, and that the positive loadings tend to correspond to values that describe shape. This suggests that some of the variation is explainable by nuclei that don't have a correlating size and shape--e.g. a large cell that exhibits low smoothness.










